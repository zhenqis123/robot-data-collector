params:
  name: "ppo_onestep"
  log_dir: './runs_ppo' 
  
  is_vision: False
  policy: # only works for MlpPolicy right now
    backbone_type: 'pn'
    freeze_backbone: False
    pi_hid_sizes: [1024, 1024, 512, 512]
    vf_hid_sizes: [1024, 1024, 512, 512]
    activation: elu # can be elu, relu, selu, crelu, lrelu, tanh, sigmoid
    pc_shape: [512, 3] # point cloud shape
    pc_emb_dim: 128

  test: ${...test}
  resume: 0
  # check for potential saves every this many iterations
  save_interval: 100 # 500
  print_log: True

  # rollout params
  max_iterations: 20000

  # training params
  cliprange: 0.2
  ent_coef: 0
  nsteps: 1
  noptepochs: 5
  nminibatches: 4 # this is per agent
  max_grad_norm: 1
  optim_stepsize: 3.e-4 # 3e-4 is default for single agent training with constant schedule
  schedule: adaptive # could be adaptive or linear or fixed
  desired_kl: 0.016
  gamma: 0.96
  lam: 0.95
  init_noise_std: 0.8

  surrogate_loss_coef: 1.0
  value_loss_coef: 2.0
  